{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:01:49.453128Z",
     "iopub.status.busy": "2023-02-12T08:01:49.452866Z",
     "iopub.status.idle": "2023-02-12T08:01:53.159005Z",
     "shell.execute_reply": "2023-02-12T08:01:53.158165Z",
     "shell.execute_reply.started": "2023-02-12T08:01:49.453111Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!pip install python-arango==7.2.0\n",
    "!pip install openai\n",
    "!pip install git+https://github.com/salesforce/LAVIS.git\n",
    "!pip install sumy\n",
    "!pip install word2number\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#!pip install nebula3_database\n",
    "#!mkdir /notebooks/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:02:43.717326Z",
     "iopub.status.busy": "2023-02-12T08:02:43.716581Z",
     "iopub.status.idle": "2023-02-12T08:02:43.720473Z",
     "shell.execute_reply": "2023-02-12T08:02:43.719930Z",
     "shell.execute_reply.started": "2023-02-12T08:02:43.717304Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/notebooks/cache/'\n",
    "import torch\n",
    "from PIL import Image\n",
    "import openai\n",
    "from arango import ArangoClient\n",
    "import requests # request img from web\n",
    "import shutil # save img locally\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "from word2number import w2n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add your api key \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:52:40.733190Z",
     "iopub.status.busy": "2023-02-12T08:52:40.733008Z",
     "iopub.status.idle": "2023-02-12T08:52:40.736125Z",
     "shell.execute_reply": "2023-02-12T08:52:40.735698Z",
     "shell.execute_reply.started": "2023-02-12T08:52:40.733175Z"
    }
   },
   "outputs": [],
   "source": [
    "dbname = \"web_demo\"\n",
    "arango_host = \"http://172.83.9.249:8529\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLIP2():\n",
    "    def __init__(self):\n",
    "        #os.environ['TRANSFORMERS_CACHE'] = '/notebooks/cache/'\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "        self.question_model, self.question_vis_processors, question_text_processors = load_model_and_preprocess(name=\"blip2_t5\", model_type=\"pretrain_flant5xxl\", is_eval=True, device=self.device)\n",
    "        #self.match_model, self.match_vis_processors, self.match_text_processors = load_model_and_preprocess(\"blip_image_text_matching\", \"large\", device=self.device, is_eval=True)\n",
    "        self.match_model, self.match_vis_processors, self.match_text_processors = load_model_and_preprocess(\"blip2_image_text_matching\", \"pretrain\", device=self.device, is_eval=True)\n",
    "        # load_model_and_preprocess(\"blip2_image_text_matching\", \"pretrain\", device=device, is_eval=True)\n",
    "class BLIP2GPTDialog():\n",
    "    def __init__(self, db_name, db_host, blip):\n",
    "        #os.environ['TRANSFORMERS_CACHE'] = '/notebooks/cache/'\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "        client = ArangoClient(hosts=db_host)\n",
    "        self.db = client.db(db_name, username='nebula', password='nebula')\n",
    "        self.prompt_step1 = \"\"\"The text below is an inaccurate image caption. \n",
    "        Ask 15 different clarifying questions to get more accurate information that can be seen in the image:\n",
    "        \n",
    "        \"\"\"\n",
    "        self.prompt_step1_1 = \"\"\"The text below is an image caption. \n",
    "        Ask 5 different clarifying questions to get more details that can be seen in the image:\n",
    "        \n",
    "        \"\"\"\n",
    "        #self.prompt_step2 = \"Text below With the provided weak and inaccurate captions for the image, taking into account clarifying questions and answers to them, write more accurate detailed captions for the same image: \\n\\n\"\n",
    "        self.prompt_step2 = \"\"\"The following text contains weak descriptions of the image, which may contain incorrect, inaccurate or irrelevant information. \n",
    "        Taking in account clarifying questions and answers about this image, provide an accurate and detailed captions of it: \n",
    "        \n",
    "        \"\"\"\n",
    "        self.question_model =  blip.question_model\n",
    "        self.question_vis_processors = blip.question_vis_processors\n",
    "        self.match_model = blip.match_model\n",
    "        self.match_vis_processors = blip.match_vis_processors\n",
    "        self.match_text_processors = blip.match_text_processors\n",
    "    \n",
    "    def sumy_sum(self, captions, count):\n",
    "        LANGUAGE = \"english\"\n",
    "        SENTENCES_COUNT = int(count)\n",
    "        prompt = []\n",
    "        parser = PlaintextParser.from_string(captions, Tokenizer(LANGUAGE))    \n",
    "        stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "        summarizer = Summarizer(stemmer)\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "        for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "            prompt.append(str(sentence))\n",
    "        return(prompt)\n",
    "\n",
    "    def get_mdfs(self, movie_id):\n",
    "        mdfs = []\n",
    "        for res in self.db.collection(\"s4_llm_output\").find({'movie_id': movie_id}):\n",
    "            mdfs.append(res)\n",
    "        newlist = sorted(mdfs, key=lambda d: d['frame_num'])\n",
    "        return(newlist)\n",
    "    \n",
    "    def download_file(self, url):\n",
    "        file_name = \"/tmp/\" + url.split(\"/\")[-1] #prompt user for file_name\n",
    "        #print(file_name)\n",
    "        res = requests.get(url, stream = True)\n",
    "\n",
    "        if res.status_code == 200:\n",
    "            with open(file_name,'wb') as f:\n",
    "                shutil.copyfileobj(res.raw, f)\n",
    "            print('Image sucessfully Downloaded: ',file_name)\n",
    "        else:\n",
    "            print('Image Couldn\\'t be retrieved')\n",
    "        return(file_name)\n",
    "    \n",
    "    def ask_gpt(self, prompt, text):\n",
    "        prompt = prompt + \"\\n\"\n",
    "        prompt = prompt + text\n",
    "        try:\n",
    "            response = openai.Completion.create(\n",
    "            engine=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            temperature=0.0,\n",
    "            max_tokens=256,\n",
    "            #top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0\n",
    "            )\n",
    "            #print(response['choices'][0]['text'])\n",
    "            return(response['choices'][0]['text'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return(\"No answer from OpenAI, please re-try in few minutes\")\n",
    "    \n",
    "    def process_image_and_captions(self, movie_id, frame):\n",
    "        mdfs = {}\n",
    "        for mdf in self.get_mdfs(movie_id):\n",
    "            if mdf['frame_num'] == frame:\n",
    "                mdfs = mdf\n",
    "        mdf_file = self.download_file(mdfs['url']) \n",
    "        # load sample image\n",
    "        raw_image = Image.open(mdf_file).convert(\"RGB\")\n",
    "        display(raw_image.resize((596, 437)))\n",
    "        question_image = self.question_vis_processors[\"eval\"](raw_image).unsqueeze(0).to(self.device)\n",
    "        match_image = self.match_vis_processors[\"eval\"](raw_image).unsqueeze(0).to(self.device)\n",
    "        all_captions = \"\" \n",
    "        \n",
    "        for para in mdfs['paragraphs']:\n",
    "            #print(para)a\n",
    "            all_captions = all_captions + \" \" + para\n",
    "        all_captions = mdfs['candidate'] + \" \" + all_captions\n",
    "        sum_captions = self.sumy_sum(all_captions, 30) \n",
    "        #print(sum_captions)\n",
    "        captions = \"\" \n",
    "        for sentence in sum_captions:\n",
    "            #for sentence in sentences.split(\". \"):          \n",
    "            txt = self.match_text_processors[\"eval\"](sentence)\n",
    "            itm_output = self.match_model({\"image\": match_image, \"text_input\": txt}, match_head=\"itm\")\n",
    "            itm_scores = torch.nn.functional.softmax(itm_output, dim=1)\n",
    "            if  itm_scores[:, 1].item() >= 0.3:\n",
    "                captions = captions +  sentence + \"\\n\"\n",
    "                print(\"Good \", sentence, \" \", itm_scores[:, 1].item())\n",
    "            else:\n",
    "                print(\"Bad \", sentence, \" \", itm_scores[:, 1].item())\n",
    "        return(question_image, match_image, captions, all_captions)\n",
    "\n",
    "    def get_questions(self, captions):\n",
    "        #Human related question -> ask only if there are people in the image\n",
    "        human_related = []\n",
    "        human_related.append(\"Question: How many people are in the picture? Answer: \")\n",
    "        human_related.append(\"Question: How are the people in the picture different from each other? Answer:\")\n",
    "        human_related.append(\"Question: What outerwear are the people in the picture wearing?? Answer:\")\n",
    "        human_related.append(\"Question: How old is the man shown first from the right in the picture? Answer:\")\n",
    "        human_related.append(\"Question: How old is the man shown second from the right in the picture? Answer:\")\n",
    "        human_related.append(\"Question: What color is the outerwear of the people in the picture? Answer:\")\n",
    "        human_related.append(\"Question: What outerwear do the people in the picture have? Answer:\")\n",
    "        human_related.append(\"Question: What is the person shown first from the right in the picture doing? Answer:\")\n",
    "        human_related.append(\"Question: What is the person shown second from the right in the picture doing? Answer:\")\n",
    "        human_related.append(\"Question: What is the moods of the people in the picture? Answer:\")\n",
    "        human_related.append(\"Question: What are the distinguishing features of people in the picture? Answer:\")\n",
    "        #Common questions\n",
    "        general_prompts = []  \n",
    "        general_prompts.append(\"Question: Where the image is taken in? Answer: \")\n",
    "        general_prompts.append(\"Question: What inanimate objects are in the picture? Answer:\")\n",
    "        general_prompts.append(\"Question: Is there something unusual in the picture? Answer:\")\n",
    "        general_prompts.append(\"Question: What is backgound in the picture? Answer:\")\n",
    "        #Ask GPT-3 for all possible questions, according to captions we have\n",
    "        captions = \"Captions: \\n\" + captions + \"\\n\"\n",
    "        result  = self.ask_gpt(self.prompt_step1, captions)\n",
    "        questions = ''.join([i for i in result if not i.isdigit()])\n",
    "        questions = questions.split(\".\") \n",
    "        for question in questions[1::]:\n",
    "            question = question.replace(\"\\n\",\"\") \n",
    "            if question:\n",
    "                general_prompts.append(\"Question: \"  + question.replace(\"\\n\",\"\") + \" Answer:\")\n",
    "                #print(\"Question: \" + question + \" Answer:\")   \n",
    "        return(general_prompts, human_related)\n",
    "    \n",
    "    def get_answers(self, generated_questions, human_related, question_image):\n",
    "        answers = \"\"  \n",
    "        have_people = \"Question: Are there people in the picture? Answer: \"\n",
    "        answer =  self.question_model.generate({\"image\": question_image, \"prompt\": have_people})\n",
    "        if answer[0] == \"yes\":\n",
    "            for question in human_related:\n",
    "                prompt = answers + question\n",
    "                answer =  self.question_model.generate({\"image\": question_image, \"prompt\": prompt})\n",
    "                #why_question = question + answer[0] + \"Question: Why? Answer:\"\n",
    "                #why_answer =  self.question_model.generate({\"image\": question_image, \"prompt\": why_question})\n",
    "                answers = answers + question + \" \" + answer[0] + \"\\n\" \n",
    "                #print(\"DDDD \", prompt, \" \", answer) \n",
    "                #answers = answers + question + \" \" + answer[0] + \"\\n\"    \n",
    "                #print(\"human \",answer)\n",
    "        for question in generated_questions:\n",
    "            prompt = answers + question\n",
    "            answer =  self.question_model.generate({\"image\": question_image, \"prompt\": prompt})\n",
    "            #print(\"DDDD\", prompt, \" \", answer)\n",
    "            if answer[0] != \"no\":\n",
    "                #why_question = question + answer[0] + \"Question: Why? Answer:\"\n",
    "                #why_answer =  self.question_model.generate({\"image\": question_image, \"prompt\": why_question})\n",
    "                answers = answers + question + \" \" + answer[0] + \"\\n\" \n",
    "            #print(\"general \", answer)\n",
    "        answers = answers + \"\\n\"\n",
    "        return(answers)\n",
    "\n",
    "    def get_dialog_caption(self, movie_id, frame_num):\n",
    "        question_image, match_image, captions, all_captions = self.process_image_and_captions(movie_id, frame_num)\n",
    "        #print(\"BEFORE: \", captions)\n",
    "        questions, human_related = self.get_questions(captions)\n",
    "        answers = self.get_answers(questions, human_related, question_image)\n",
    "        questions_step2 = \"Original weak captions: \\n\" + captions + \"\\n\" + \"Clarifying Questions and Answers: \\n\"+ answers + \"\\n\" + \"Accurate captions:\"\n",
    "        print(questions_step2)\n",
    "        final_results  = self.ask_gpt(self.prompt_step2, questions_step2)\n",
    "        return(final_results)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_mdfs(movie_id, db))\n",
    "blip = BLIP2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog = BLIP2GPTDialog(dbname, arango_host, blip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id = \"Movies/-6674768436086825782\"\n",
    "frame_num = 0\n",
    "results = dialog.get_dialog_caption(movie_id, frame_num)\n",
    "print('\\n' + results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "597985ec3a85199db6d51c1acd236759121adbd34d8abd143f6cf1ec68ea0309"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
